{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "加法注意の実装\n",
    "\n",
    "NEURAL MACHINE TRANSLATION\n",
    "BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\n",
    "\n",
    "[Dzmitry Bahdanau, sec: Kyunghyun Cho, last: Yoshua Bengio, ICLR 2015, arXiv, 2014/09]\n",
    "\n",
    "https://bastings.github.io/annotated_encoder_decoder/\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"\n",
    "        maskされたソースとtarget 列　を取り込み、処理する\n",
    "        \"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask, decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden,\n",
    "                            encoder_final, src_mask, trg_mask, hidden=decoder_hidden)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    linear model と softmax を定義する\n",
    "    fainal output の次元はtarget vocabulary のsize\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([[[1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.]]]) \n",
      "\n",
      "h0 tensor([[[1., 1.]]]) \n",
      "\n",
      "output tensor([[[0.8479, 0.4703]],\n",
      "\n",
      "        [[0.7671, 0.4122]],\n",
      "\n",
      "        [[0.7190, 0.3987]],\n",
      "\n",
      "        [[0.6894, 0.3945]]], grad_fn=<CatBackward>)\n",
      "hidden state tensor([[[0.6894, 0.3945]]], grad_fn=<ViewBackward>)\n",
      "Parameter containing:\n",
      "tensor([[-0.3301,  0.1802],\n",
      "        [-0.3258, -0.0829],\n",
      "        [-0.2872,  0.4691],\n",
      "        [-0.5582, -0.3260],\n",
      "        [-0.1997, -0.4252],\n",
      "        [ 0.0667, -0.6984]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5406,  0.5869, -0.1657],\n",
      "        [ 0.6496, -0.1549,  0.1427],\n",
      "        [-0.3443,  0.4153,  0.6233],\n",
      "        [-0.5188,  0.6146,  0.1323],\n",
      "        [ 0.5224,  0.0958,  0.3410],\n",
      "        [-0.0998,  0.5451,  0.1045]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "hidden_size=2\n",
    "gru = nn.GRU(input_size=3, hidden_size=hidden_size, num_layers=1,bias=False)\n",
    "input = torch.ones(4,1,3)\n",
    "h0=torch.ones(1,1,hidden_size)\n",
    "print('input',input,'\\n')\n",
    "print('h0', h0, '\\n')\n",
    "print('output',gru(input, h0)[0])\n",
    "print('hidden state',gru(input, h0)[1])\n",
    "print(gru.weight_hh_l0)\n",
    "print(gru.weight_ih_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2072,  0.7001, -0.0920], grad_fn=<SelectBackward>)\n",
      "tensor([[[1.],\n",
      "         [1.]]])\n",
      "tensor([-0.2072,  0.7001, -0.0920], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(gru.weight_ih_l0[0])\n",
    "print(h0)\n",
    "print(gru.weight_ih_l0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dopout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        embeddings x に bidirectional GRU を適用する\n",
    "        input の mini-batch x は lengthでソートされる必要がある\n",
    "        x は [batch, time, dim] をもつ\n",
    "        \n",
    "        pytorch では　pack_padded_sequence, pad_packed_sequence という\n",
    "        ２つの関数がmask と padding を請け負ってくれる\n",
    "        pack_padded_sequence -> padされた系列データの各バッチを時刻が早い順にpackしてくれる\n",
    "        \n",
    "        length : Tのこと時系列の長さ\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        # 双方向のfinal hidden statesを結合させる\n",
    "        \n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2) #[num_layers, batch, 2*dim]\n",
    "        return output, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-246db5b39685>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-246db5b39685>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,,\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# conditional GRU: 隠れ層の第一相をencoderの最終層に使うGRU のこと？\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,,\n",
    "                bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # nn.GRU (input_size, hidden_size, num_layers) input_size:入力の次元数, hidden_size: 隠れ層の次元数, numl_layers: スタック数\n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        # encoderのfinal hidden層 を initialize する. パラメーターをdecoder用に変換している（引数に注目）\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\" perfrom a single decoder step (1word) \"\"\"\n",
    "        \n",
    "        # compute context vextor usinf attention mechanism\n",
    "        query = hidden[-1].unsqueeze(1) # [#layers, B, D] -> [B, 1, D] # decoderの最終層(スタック方向)の隠れ層を取り出してqueryとする\n",
    "        context, attn_probs = self.attention(\n",
    "        query=query, proj_key=proj_key,\n",
    "            value = encoder_hidden, mask=src_mask )\n",
    "        \n",
    "        # update rnn hidden state\n",
    "        # p13 の s˜iの式 #入力系列 [batch_size, seq_length, feature_size] 今 seq_length=1\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        pre_output = torch.cat([prev_embed, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "        \n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, src_mask, trg_mask,\n",
    "                hidden=None, max_len=None):\n",
    "        \n",
    "        # RNN の 時系列方向の最大長\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "            \n",
    "        proj_key = self.attention.key_layer(encoder_hidden) # encoder_hidden_size を decoder_hidden_sizeにしている\n",
    "        \n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        # decoder RNN を max_len まで 展開する\n",
    "        for i in range(max_len):\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1) # trg_embed[:, i, :] と同じ [A, 1, B] となる => seq_length=1\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "            prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "            \n",
    "            decoder_states = torch.cat(decder_states, dim=1)\n",
    "            pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "            return decoder_states, hidden, pre_output_vectors \n",
    "            \n",
    "        def init_hidden(self, encoder_final):\n",
    "            if encoder_final is None:\n",
    "                return None \n",
    "            \n",
    "            return torch.tanh(self.bridge(encder_final))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2858,  0.4943,  1.2176, -0.8914,  0.7860,  1.0060,  0.8448, -0.1627,\n",
      "         1.3187,  0.5707])\n",
      "tensor([[-1.2858,  0.4943,  1.2176, -0.8914,  0.7860,  1.0060,  0.8448, -0.1627,\n",
      "          1.3187,  0.5707]])\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Bahdnau(MLP) attention\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # bi-directional encoder なので key_size = 2 * hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size None else query_size\n",
    "        \n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention score\n",
    "        self.alpha = None\n",
    "    \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "        # we first project the query\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # calculate socre\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key)) # query: decoderの隠れ層si-1 proj_key: encoder_hiddenをdecoder_hidden_sizeにしたもの\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions\n",
    "        # The mask marks valid positions so we invert it using 'mask & 0'\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas\n",
    "        context = torch.bmm(alphas, value) # 重みとエンコーダーの隠れ層の各要素の和＝内積\n",
    "        \n",
    "        return context, alphas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5065,  0.4956, -0.2470, -1.2075],\n",
      "         [-0.7373,  1.1051,  0.8750,  1.1536],\n",
      "         [ 1.8955,  1.4530,  0.6975,  1.6495]]])\n",
      "tensor([[[ 0.1721,  0.5236, -0.7787,  1.1389],\n",
      "         [-1.1370,  0.5588,  1.3614, -0.8475],\n",
      "         [ 1.0006, -0.9063,  0.6109,  0.9280]]])\n",
      "tensor([[[-1.5065,  0.4956, -0.2470, -1.2075,  0.1721,  0.5236, -0.7787,\n",
      "           1.1389],\n",
      "         [-0.7373,  1.1051,  0.8750,  1.1536, -1.1370,  0.5588,  1.3614,\n",
      "          -0.8475],\n",
      "         [ 1.8955,  1.4530,  0.6975,  1.6495,  1.0006, -0.9063,  0.6109,\n",
      "           0.9280]]])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 1, 10])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((1,3,4))\n",
    "b = torch.randn((1,3,4))\n",
    "c = torch.cat([a,b],dim=2)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "d = torch.randn((10,10))\n",
    "print(d.shape)\n",
    "print(d[:,5].shape)\n",
    "print(d.unsqueeze(1).shape)\n",
    "print(a[:,1,:].shape)\n",
    "print(a[:,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
