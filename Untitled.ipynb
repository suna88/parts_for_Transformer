{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        \"\"\"\n",
    "        maskされたソースとtarget 列　を取り込み、処理する\n",
    "        \"\"\"\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask, decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden,\n",
    "                            encoder_final, src_mask, trg_mask, hidden=decoder_hidden)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    linear model と softmax を定義する\n",
    "    fainal output の次元はtarget vocabulary のsize\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dopout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        embeddings x に bidirectional GRU を適用する\n",
    "        input の mini-batch x は lengthでソートされる必要がある\n",
    "        x は [batch, time, dim] をもつ\n",
    "        \n",
    "        pytorch では　pack_padded_sequence, pad_packed_sequence という\n",
    "        ２つの関数がmask と padding を請け負ってくれる\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        # 双方向のfinal hidden statesを結合させる\n",
    "        fwd_final = final[0:final.size(0):2]\n",
    "        bwd_final = final[1:final.size(0):2]\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2) #[num_layers, batch, 2*dim]\n",
    "        return output, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4790,  0.4323, -0.1250,  0.7821,  0.5635,  1.8582],\n",
       "        [ 1.0441, -0.8638,  1.3059,  0.2466, -1.9776,  0.0179],\n",
       "        [-1.4129, -1.8791, -0.1798,  0.7904, -0.1222, -0.7470],\n",
       "        [ 1.7093,  0.0579,  0.5230,  0.9717, -0.2779, -0.6116],\n",
       "        [-0.5572, -0.9683,  0.8713, -0.0956,  0.3463, -0.5402],\n",
       "        [ 0.8569, -0.6721,  1.0682, -0.2527, -0.1882, -0.7712]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn((6,6))\n",
    "b=a[0:a.size(0):2]\n",
    "c=a[1:a.size(0):2]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4790,  0.4323, -0.1250,  0.7821,  0.5635,  1.8582],\n",
       "        [-1.4129, -1.8791, -0.1798,  0.7904, -0.1222, -0.7470],\n",
       "        [-0.5572, -0.9683,  0.8713, -0.0956,  0.3463, -0.5402]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0441, -0.8638,  1.3059,  0.2466, -1.9776,  0.0179],\n",
       "        [ 1.7093,  0.0579,  0.5230,  0.9717, -0.2779, -0.6116],\n",
       "        [ 0.8569, -0.6721,  1.0682, -0.2527, -0.1882, -0.7712]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
