{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, wx, wh, b):\n",
    "        self.params = [wx, wh, b]\n",
    "        self.grads = [np.zeros_like(wx), np.zeros_like(wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        wx, wh, b = self.params\n",
    "        t = np.dot(h_prev, wh)+np.dot(x, wx)+b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        wx, wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dwh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, wh.T)\n",
    "        dwx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, wx.T)\n",
    "        \n",
    "        self.grads[0][...] = dwx\n",
    "        self.grads[1][...] = dwh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, wx, wh, b, stateful=False):\n",
    "        self.params = [wx, wh, b]\n",
    "        self.grads = [np.zeros_like(wx), np.zeros_like(wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        wx, wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            self. h = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params) # * はリストなどを分解して渡すことを意味する\n",
    "            self.h = layer.forward(xs[:, t, :], self.h) # h_prevから時刻tのｈを作成\n",
    "            hs[:, t, :] = self.h # 時刻tのhを格納\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        wx, wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh) # 合算した勾配\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                self.grads[i] += grad\n",
    "        \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        \n",
    "        return dxs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "\n",
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 重みの初期化(Xavierの初期値)\n",
    "        embed_w = (rn(V, D) / 100).astype('f')\n",
    "        rnn_wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_w = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_w),\n",
    "            TimeRNN(rnn_wx, rnn_wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_w, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "        \n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 1000 , vocabulary size: 418\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "from common.optimizer import SGD\n",
    "from dataset import ptb\n",
    "# from simple_rnnlm import SimpleRnnlm\n",
    "\n",
    "# ハイパーパラメーターの設定\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 5 # Truncated BPTT の展開する時間サイズ\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "#学習データの読み込み（データセットを小さくする）\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "data_size = len(xs)\n",
    "print('corpus size: {} , vocabulary size: {}'.format(corpus_size, vocab_size))\n",
    "\n",
    "# 学習時に使用する変数\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "# モデルの生成\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチの各サンプルの読み込み開始位置を計算\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "[0, 99, 198, 297, 396, 495, 594, 693, 792, 891]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(jump)\n",
    "print(offsets)\n",
    "print(max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | perplexity 319.9095499909084\n",
      "| epoch 2 | perplexity 228.0625413561379\n",
      "| epoch 3 | perplexity 214.70397216620427\n",
      "| epoch 4 | perplexity 204.60185065157677\n",
      "| epoch 5 | perplexity 199.6166219409499\n",
      "| epoch 6 | perplexity 200.82618526058175\n",
      "| epoch 7 | perplexity 196.58991923104267\n",
      "| epoch 8 | perplexity 194.8412684659621\n",
      "| epoch 9 | perplexity 189.49757977914103\n",
      "| epoch 10 | perplexity 188.3140973342415\n",
      "| epoch 11 | perplexity 192.55164664566792\n",
      "| epoch 12 | perplexity 186.82416470488226\n",
      "| epoch 13 | perplexity 190.71239448869727\n",
      "| epoch 14 | perplexity 186.85053113644756\n",
      "| epoch 15 | perplexity 185.91763971110217\n",
      "| epoch 16 | perplexity 185.14550844938717\n",
      "| epoch 17 | perplexity 187.19349825616453\n",
      "| epoch 18 | perplexity 182.36828188246133\n",
      "| epoch 19 | perplexity 178.39877713840505\n",
      "| epoch 20 | perplexity 175.81297013296353\n",
      "| epoch 21 | perplexity 174.76558524096808\n",
      "| epoch 22 | perplexity 177.17941461370566\n",
      "| epoch 23 | perplexity 177.17253351813707\n",
      "| epoch 24 | perplexity 174.71681649535972\n",
      "| epoch 25 | perplexity 169.59560281655627\n",
      "| epoch 26 | perplexity 169.12738574098302\n",
      "| epoch 27 | perplexity 164.22728181675706\n",
      "| epoch 28 | perplexity 165.23883888261102\n",
      "| epoch 29 | perplexity 156.97268502313133\n",
      "| epoch 30 | perplexity 152.57693413215628\n",
      "| epoch 31 | perplexity 151.72244336232126\n",
      "| epoch 32 | perplexity 149.6704957654122\n",
      "| epoch 33 | perplexity 147.26179158490066\n",
      "| epoch 34 | perplexity 140.2176892357835\n",
      "| epoch 35 | perplexity 136.344233480017\n",
      "| epoch 36 | perplexity 135.0398049558234\n",
      "| epoch 37 | perplexity 130.95012284324855\n",
      "| epoch 38 | perplexity 126.06848872627633\n",
      "| epoch 39 | perplexity 119.4746405181237\n",
      "| epoch 40 | perplexity 116.36100656950674\n",
      "| epoch 41 | perplexity 110.71424167510087\n",
      "| epoch 42 | perplexity 108.5387171755321\n",
      "| epoch 43 | perplexity 106.0063349911708\n",
      "| epoch 44 | perplexity 101.40500113759462\n",
      "| epoch 45 | perplexity 95.70572089812026\n",
      "| epoch 46 | perplexity 92.56489922008797\n",
      "| epoch 47 | perplexity 88.72783495337603\n",
      "| epoch 48 | perplexity 86.59160252306951\n",
      "| epoch 49 | perplexity 83.28241214873928\n",
      "| epoch 50 | perplexity 74.73436872271965\n",
      "| epoch 51 | perplexity 72.65732784548622\n",
      "| epoch 52 | perplexity 70.1417439444219\n",
      "| epoch 53 | perplexity 67.47628503665011\n",
      "| epoch 54 | perplexity 64.18894853557498\n",
      "| epoch 55 | perplexity 60.3115505420523\n",
      "| epoch 56 | perplexity 58.33573972942217\n",
      "| epoch 57 | perplexity 55.981566705483445\n",
      "| epoch 58 | perplexity 51.53023670061734\n",
      "| epoch 59 | perplexity 49.08551530615984\n",
      "| epoch 60 | perplexity 45.49329069741977\n",
      "| epoch 61 | perplexity 42.32439870681115\n",
      "| epoch 62 | perplexity 41.01446797039848\n",
      "| epoch 63 | perplexity 40.044916422477776\n",
      "| epoch 64 | perplexity 38.584151922006114\n",
      "| epoch 65 | perplexity 33.75176205177578\n",
      "| epoch 66 | perplexity 33.336005901819796\n",
      "| epoch 67 | perplexity 31.595696830138372\n",
      "| epoch 68 | perplexity 29.92643086772809\n",
      "| epoch 69 | perplexity 27.88148787730768\n",
      "| epoch 70 | perplexity 26.213576558794255\n",
      "| epoch 71 | perplexity 24.101731359437235\n",
      "| epoch 72 | perplexity 23.127556124355596\n",
      "| epoch 73 | perplexity 22.766328515617708\n",
      "| epoch 74 | perplexity 21.41790483484446\n",
      "| epoch 75 | perplexity 19.903602437960917\n",
      "| epoch 76 | perplexity 19.024449221885998\n",
      "| epoch 77 | perplexity 18.3447145009009\n",
      "| epoch 78 | perplexity 17.084885822293767\n",
      "| epoch 79 | perplexity 16.62170369275753\n",
      "| epoch 80 | perplexity 15.137369062005195\n",
      "| epoch 81 | perplexity 14.374890781695832\n",
      "| epoch 82 | perplexity 13.87912252133904\n",
      "| epoch 83 | perplexity 13.430960202550404\n",
      "| epoch 84 | perplexity 13.434689553016208\n",
      "| epoch 85 | perplexity 11.756796485606435\n",
      "| epoch 86 | perplexity 10.744017026844407\n",
      "| epoch 87 | perplexity 10.545121031974636\n",
      "| epoch 88 | perplexity 10.1338531964385\n",
      "| epoch 89 | perplexity 10.219638150253424\n",
      "| epoch 90 | perplexity 9.187378296431225\n",
      "| epoch 91 | perplexity 8.945075543553315\n",
      "| epoch 92 | perplexity 8.187213266699924\n",
      "| epoch 93 | perplexity 7.707040756388209\n",
      "| epoch 94 | perplexity 7.340982458162315\n",
      "| epoch 95 | perplexity 6.950531130365524\n",
      "| epoch 96 | perplexity 6.587430089204811\n",
      "| epoch 97 | perplexity 6.451682599128446\n",
      "| epoch 98 | perplexity 6.310419502209751\n",
      "| epoch 99 | perplexity 6.002095270942144\n",
      "| epoch 100 | perplexity 5.8509349332783325\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "        \n",
    "        # 勾配を求め、パラメーターを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "    #エポック毎にパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch {} | perplexity {}'.format(epoch+1, ppl))\n",
    "    total_loss, loss_count = 0, 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
